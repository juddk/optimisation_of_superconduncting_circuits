{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 10000])\n",
      "torch.Size([10000, 10000])\n",
      "torch.Size([10000, 10000])\n",
      "torch.Size([10000, 10000])\n",
      "torch.Size([10000, 10000])\n",
      "torch.Size([10000, 10000])\n",
      "torch.Size([10000, 10000])\n",
      "torch.Size([10000, 10000])\n",
      "torch.Size([10000, 10000])\n",
      "tensor(indices=tensor([[   0,    1,    2,  ..., 9997, 9998, 9999],\n",
      "                       [   0,    1,    2,  ..., 9997, 9998, 9999]]),\n",
      "       values=tensor([0.4000, 0.4000, 0.4000,  ..., 0.4000, 0.4000, 0.4000]),\n",
      "       size=(10000, 10000), nnz=10000, layout=torch.sparse_coo,\n",
      "       grad_fn=<MulBackward0>)\n",
      "torch.Size([10000, 10000])\n",
      "tensor(indices=tensor([[   0,    0,    0,  ..., 9999, 9999, 9999],\n",
      "                       [   0,    1,    2,  ..., 9997, 9998, 9999]]),\n",
      "       values=tensor([-39.6000+8.j, -39.6000-8.j,   0.4000+0.j,  ...,\n",
      "                        0.4000+0.j,   0.4000+0.j, -39.6000+8.j]),\n",
      "       size=(10000, 10000), nnz=100000000, layout=torch.sparse_coo,\n",
      "       grad_fn=<MulBackward0>)\n",
      "torch.Size([10000, 10000])\n",
      "tensor(indices=tensor([[   0,    1,    2,  ..., 9997, 9998, 9999],\n",
      "                       [   0,    1,    2,  ..., 9997, 9998, 9999]]),\n",
      "       values=tensor([19.3782, 19.3782, 19.3782,  ..., 19.3782, 19.3782,\n",
      "                      19.3782]),\n",
      "       size=(10000, 10000), nnz=10000, dtype=torch.float64,\n",
      "       layout=torch.sparse_coo, grad_fn=<MulBackward0>)\n",
      "torch.Size([10000, 10000])\n",
      "tensor(indices=tensor([[ 100,  101,  102,  ..., 9997, 9998, 9999],\n",
      "                       [ 100,  101,  102,  ..., 9997, 9998, 9999]]),\n",
      "       values=tensor([1.6112e-04, 1.6112e-04, 1.6112e-04,  ...,\n",
      "                      1.5791e+00, 1.5791e+00, 1.5791e+00]),\n",
      "       size=(10000, 10000), nnz=9900, dtype=torch.float64,\n",
      "       layout=torch.sparse_coo, grad_fn=<MulBackward0>)\n",
      "torch.Size([10000, 10000])\n",
      "tensor(indices=tensor([[   0,    1,    2,  ..., 9997, 9998, 9999],\n",
      "                       [   0,    1,    2,  ..., 9997, 9998, 9999]]),\n",
      "       values=tensor([20., 20., 20.,  ..., 20., 20., 20.]),\n",
      "       size=(10000, 10000), nnz=10000, dtype=torch.float64,\n",
      "       layout=torch.sparse_coo, grad_fn=<MulBackward0>)\n",
      "torch.Size([10000, 10000])\n",
      "tensor(indices=tensor([[ 100,  101,  102,  ..., 9997, 9998, 9999],\n",
      "                       [ 100,  101,  102,  ..., 9997, 9998, 9999]]),\n",
      "       values=tensor([-0., -0., -0.,  ..., 0., 0., 0.]),\n",
      "       size=(10000, 10000), nnz=9900, dtype=torch.float64,\n",
      "       layout=torch.sparse_coo, grad_fn=<MulBackward0>)\n",
      "torch.Size([10000, 10000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[   0,    0,    1,  ..., 9997, 9998, 9999],\n",
       "                       [   0,    1,    1,  ..., 9998, 9999, 9900]]),\n",
       "       values=tensor([-30.5782, -40.0000, -30.5782,  ...,  -8.0000,\n",
       "                       -8.0000,  -8.0000]),\n",
       "       size=(10000, 10000), nnz=30000, dtype=torch.float64,\n",
       "       layout=torch.sparse_coo, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy as sp\n",
    "import scipy.sparse as sps\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def scipy_sparse_to_torch_sparse(matrix):\n",
    "    \"\"\"Converts a scipy.sparse matrix to a PyTorch sparse tensor.\"\"\"\n",
    "    if not sps.isspmatrix_coo(matrix):\n",
    "        matrix = matrix.tocoo()\n",
    "\n",
    "    values = matrix.data\n",
    "    indices = np.vstack((matrix.row, matrix.col))\n",
    "\n",
    "    i = torch.LongTensor(indices)\n",
    "    v = torch.FloatTensor(values)\n",
    "    shape = matrix.shape\n",
    "\n",
    "    return torch.sparse.FloatTensor(i, v, torch.Size(shape))\n",
    "\n",
    "EJ = torch.tensor(10.00, requires_grad=True, dtype=torch.double)\n",
    "EL = torch.tensor(0.04, requires_grad=True, dtype=torch.double)\n",
    "ECs = torch.tensor(20, requires_grad=True, dtype=torch.double)\n",
    "EC = torch.tensor(0.04, requires_grad=True, dtype=torch.double)\n",
    "dEj = torch.tensor(0.0, requires_grad=True, dtype=torch.double)\n",
    "dCj = torch.tensor(0.0, requires_grad=True, dtype=torch.double)\n",
    "ECj =  torch.tensor(0.2, requires_grad=True, dtype=torch.double)\n",
    "flux = torch.tensor(0.5, requires_grad=True, dtype=torch.double)\n",
    "ng = 0.1\n",
    "ncut = 30\n",
    "truncated_dim = 10\n",
    "pt_count = 10\n",
    "min_val = -19\n",
    "max_val = 19\n",
    "hamiltonian_creation = 'auto_H'\n",
    "\n",
    "\n",
    "phi_ext = 0.5\n",
    "varphi_ext =0.5\n",
    "\n",
    "Nphi = 100\n",
    "Ntheta = 100\n",
    "\n",
    "eye_Nphi = sps.eye(Nphi)\n",
    "eye_Ntheta = sps.eye(Ntheta)\n",
    "\n",
    "partial_phi_fd = scipy_sparse_to_torch_sparse(sps.kron(eye_Ntheta, sps.diags([-1, 1, 1], [0, 1, -Nphi+1], shape=(Nphi, Nphi))))\n",
    "\n",
    "partial_phi_bk = scipy_sparse_to_torch_sparse(sps.kron(eye_Ntheta, sps.diags([1, -1, -1], [0, -1, Nphi-1], shape=(Nphi, Nphi))))\n",
    "\n",
    "partial_theta_fd = scipy_sparse_to_torch_sparse(sps.kron(eye_Nphi, sps.diags([-1, 1, 1], [0, 1, -Ntheta+1], shape=(Ntheta, Ntheta))))\n",
    "\n",
    "partial_theta_bk = scipy_sparse_to_torch_sparse(sps.kron(eye_Nphi, sps.diags([1, -1, -1], [0, -1, Ntheta-1], shape=(Ntheta, Ntheta))))\n",
    "\n",
    "\n",
    "phi = np.linspace(0, 2 * np.pi, Nphi)\n",
    "cos_phi = np.cos(phi)\n",
    "cos_phi_m = np.diag(cos_phi)\n",
    "sin_phi_adj = np.sin(phi-phi_ext/2)\n",
    "sin_phi_adj_m = np.diag(sin_phi_adj)\n",
    "phi_m = np.diag(phi)\n",
    "_cos_phi = torch.kron(torch.tensor(cos_phi_m), torch.tensor(eye_Nphi.todense()) ).to_sparse()\n",
    "_phi = torch.kron(torch.tensor(phi_m), torch.tensor(eye_Nphi.todense()) ).to_sparse()\n",
    "_sin_phi_adj_m  = torch.kron(torch.tensor(sin_phi_adj_m), torch.tensor(eye_Nphi.todense())).to_sparse()\n",
    "\n",
    "theta = np.linspace(0, 2 * np.pi, Ntheta)\n",
    "cos_theta_adj = np.cos(theta-varphi_ext/2)\n",
    "sin_theta = np.sin(theta)\n",
    "cos_theta_adj_m = np.diag(cos_theta_adj)\n",
    "sin_theta_m = np.diag(sin_theta)\n",
    "_cos_theta_adj_m = torch.kron(torch.tensor(cos_theta_adj_m), torch.tensor(eye_Ntheta.todense())).to_sparse()\n",
    "_sin_theta_m = torch.kron(torch.tensor(sin_theta_m), torch.tensor(eye_Ntheta.todense())).to_sparse()\n",
    "\n",
    "#What do the krons do? The partial derivative operators are identical  - why would we chose different N to make them different? \n",
    "\n",
    "print(partial_phi_fd.size())\n",
    "print(partial_phi_bk.size())\n",
    "print(partial_theta_fd.size())\n",
    "print(partial_theta_bk.size())\n",
    "\n",
    "print(_cos_phi .size())\n",
    "print(_phi.size())\n",
    "print(_sin_phi_adj_m.size())\n",
    "print(_cos_theta_adj_m.size())\n",
    "print(_sin_theta_m.size())\n",
    "\n",
    "\n",
    "a = -2 * ECj * (partial_phi_fd * partial_phi_bk) \n",
    "b = 2 * ECs * ((1j* partial_theta_fd.to_dense() - ng)**2).to_sparse()\n",
    "c = 2 * EJ * _cos_phi * _cos_theta_adj_m\n",
    "d = EL * _phi ** 2\n",
    "e = 2 * EJ * torch.kron(torch.tensor(eye_Nphi.todense()), torch.tensor(eye_Ntheta.todense())).to_sparse()\n",
    "f = EJ * dEj * _sin_theta_m * _sin_phi_adj_m\n",
    "print(a)\n",
    "print(a.size())\n",
    "print(b)\n",
    "print(b.size())\n",
    "print(c)\n",
    "print(c.size())\n",
    "print(d)\n",
    "print(d.size())\n",
    "print(e)\n",
    "print(e.size())\n",
    "print(f)\n",
    "print(f.size())\n",
    "\n",
    "I = torch.kron(torch.tensor(eye_Nphi.todense()), torch.tensor(eye_Ntheta.todense())).to_sparse()\n",
    "\n",
    "H = -2 * ECj * (partial_phi_fd * partial_phi_bk) \\\n",
    "    + 2 * ECs * (-1* partial_theta_fd**2 +ng**2*I-2*ng*partial_theta_fd)\\\n",
    "    + 2 * ECs * dCj * partial_phi_fd * partial_theta_fd \\\n",
    "    - 2 * EJ * _cos_phi * _cos_theta_adj_m \\\n",
    "    + EL * _phi ** 2 \\\n",
    "    + 2 * EJ * I  \\\n",
    "    + EJ * dEj * _sin_theta_m * _sin_phi_adj_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "def pytorch_sparse_to_scipy(tensor):\n",
    "    # Ensure the tensor is sparse\n",
    "    assert tensor.is_sparse, \"Input should be a sparse tensor\"\n",
    "\n",
    "    # Get the tensor attributes\n",
    "    indices = tensor._indices().numpy()\n",
    "    values = tensor._values().numpy()\n",
    "    size = tensor.size()\n",
    "\n",
    "    # Construct the corresponding SciPy sparse matrix\n",
    "    return csr_matrix((values, indices), shape=size)\n",
    "\n",
    "x = pytorch_sparse_to_scipy(H)\n",
    "ans = sps.linalg.eigsh(x,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'grad' from 'autograd' (/Users/judd/Documents/optimisation_of_superconduncting_circuits/venv/lib/python3.8/site-packages/autograd/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcustom_autograd_vjp\u001b[39;00m  \u001b[39m# The file containing your custom gradient code\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mautograd\u001b[39;00m \u001b[39mimport\u001b[39;00m grad\n\u001b[1;32m      5\u001b[0m x \u001b[39m=\u001b[39m pytorch_sparse_to_scipy(H)\n\u001b[1;32m      6\u001b[0m ans \u001b[39m=\u001b[39m sps\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39meigsh(x,\u001b[39m6\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'grad' from 'autograd' (/Users/judd/Documents/optimisation_of_superconduncting_circuits/venv/lib/python3.8/site-packages/autograd/__init__.py)"
     ]
    }
   ],
   "source": [
    "\n",
    "import custom_autograd_vjp  # The file containing your custom gradient code\n",
    "from autograd import grad\n",
    "\n",
    "\n",
    "x = pytorch_sparse_to_scipy(H)\n",
    "ans = sps.linalg.eigsh(x,6)\n",
    "\n",
    "def sum_of_eigenvalues(matrix, k=10):\n",
    "    eigenvalues, eigenvectors = custom_autograd_vjp.eigsh_ag(matrix, k)\n",
    "    return np.sum(eigenvalues)\n",
    "\n",
    "# Gradient of the sum of the first k eigenvalues with respect to the matrix\n",
    "grad_eig = grad(sum_of_eigenvalues)\n",
    "\n",
    "# Test it on an example matrix:\n",
    "matrix = np.array([[2, -1, 0], [-1, 2, -1], [0, -1, 2]], dtype=float)\n",
    "\n",
    "# Compute gradient\n",
    "gradient_matrix = grad_eig(matrix, 3)\n",
    "print(gradient_matrix)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'autograd' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mautograd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mprint\u001b[39m(autograd\u001b[39m.\u001b[39;49m__version__)\n\u001b[1;32m      6\u001b[0m x,y  \u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlobpcg(H\u001b[39m.\u001b[39mto_dense(), k\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m )\n\u001b[1;32m      8\u001b[0m dE \u001b[39m=\u001b[39m x[\u001b[39m0\u001b[39m]\u001b[39m-\u001b[39mx[\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'autograd' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "x,y  =torch.lobpcg(H.to_dense(), k=10 )\n",
    "\n",
    "dE = x[0]-x[1]\n",
    "\n",
    "dE.backward()\n",
    "\n",
    "EJ.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'DominantSparseEigenAD'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mDominantSparseEigenAD\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msymeig\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msymeig\u001b[39;00m\n\u001b[1;32m      3\u001b[0m symeig\u001b[39m.\u001b[39msetDominantSparseSymeig(A, Aadjoint_to_padjoint)\n\u001b[1;32m      4\u001b[0m dominantsparsesymeig \u001b[39m=\u001b[39m symeig\u001b[39m.\u001b[39mDominantSparseSymeig\u001b[39m.\u001b[39mapply\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'DominantSparseEigenAD'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
