{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 10000])\n",
      "torch.Size([10000, 10000])\n",
      "torch.Size([10000, 10000])\n",
      "torch.Size([10000, 10000])\n",
      "torch.Size([10000, 10000])\n",
      "torch.Size([10000, 10000])\n",
      "torch.Size([10000, 10000])\n",
      "torch.Size([10000, 10000])\n",
      "torch.Size([10000, 10000])\n",
      "tensor(indices=tensor([[   0,    1,    2,  ..., 9997, 9998, 9999],\n",
      "                       [   0,    1,    2,  ..., 9997, 9998, 9999]]),\n",
      "       values=tensor([0.4000, 0.4000, 0.4000,  ..., 0.4000, 0.4000, 0.4000]),\n",
      "       size=(10000, 10000), nnz=10000, layout=torch.sparse_coo,\n",
      "       grad_fn=<MulBackward0>)\n",
      "torch.Size([10000, 10000])\n",
      "tensor(indices=tensor([[   0,    0,    0,  ..., 9999, 9999, 9999],\n",
      "                       [   0,    1,    2,  ..., 9997, 9998, 9999]]),\n",
      "       values=tensor([-39.6000+8.j, -39.6000-8.j,   0.4000+0.j,  ...,\n",
      "                        0.4000+0.j,   0.4000+0.j, -39.6000+8.j]),\n",
      "       size=(10000, 10000), nnz=100000000, layout=torch.sparse_coo,\n",
      "       grad_fn=<MulBackward0>)\n",
      "torch.Size([10000, 10000])\n",
      "tensor(indices=tensor([[   0,    1,    2,  ..., 9997, 9998, 9999],\n",
      "                       [   0,    1,    2,  ..., 9997, 9998, 9999]]),\n",
      "       values=tensor([19.3782, 19.3782, 19.3782,  ..., 19.3782, 19.3782,\n",
      "                      19.3782]),\n",
      "       size=(10000, 10000), nnz=10000, dtype=torch.float64,\n",
      "       layout=torch.sparse_coo, grad_fn=<MulBackward0>)\n",
      "torch.Size([10000, 10000])\n",
      "tensor(indices=tensor([[ 100,  101,  102,  ..., 9997, 9998, 9999],\n",
      "                       [ 100,  101,  102,  ..., 9997, 9998, 9999]]),\n",
      "       values=tensor([1.6112e-04, 1.6112e-04, 1.6112e-04,  ...,\n",
      "                      1.5791e+00, 1.5791e+00, 1.5791e+00]),\n",
      "       size=(10000, 10000), nnz=9900, dtype=torch.float64,\n",
      "       layout=torch.sparse_coo, grad_fn=<MulBackward0>)\n",
      "torch.Size([10000, 10000])\n",
      "tensor(indices=tensor([[   0,    1,    2,  ..., 9997, 9998, 9999],\n",
      "                       [   0,    1,    2,  ..., 9997, 9998, 9999]]),\n",
      "       values=tensor([20., 20., 20.,  ..., 20., 20., 20.]),\n",
      "       size=(10000, 10000), nnz=10000, dtype=torch.float64,\n",
      "       layout=torch.sparse_coo, grad_fn=<MulBackward0>)\n",
      "torch.Size([10000, 10000])\n",
      "tensor(indices=tensor([[ 100,  101,  102,  ..., 9997, 9998, 9999],\n",
      "                       [ 100,  101,  102,  ..., 9997, 9998, 9999]]),\n",
      "       values=tensor([-0., -0., -0.,  ..., 0., 0., 0.]),\n",
      "       size=(10000, 10000), nnz=9900, dtype=torch.float64,\n",
      "       layout=torch.sparse_coo, grad_fn=<MulBackward0>)\n",
      "torch.Size([10000, 10000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[   0,    0,    1,  ..., 9997, 9998, 9999],\n",
       "                       [   0,    1,    1,  ..., 9998, 9999, 9900]]),\n",
       "       values=tensor([-30.5782, -40.0000, -30.5782,  ...,  -8.0000,\n",
       "                       -8.0000,  -8.0000]),\n",
       "       size=(10000, 10000), nnz=30000, dtype=torch.float64,\n",
       "       layout=torch.sparse_coo, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy as sp\n",
    "import scipy.sparse as sps\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def scipy_sparse_to_torch_sparse(matrix):\n",
    "    \"\"\"Converts a scipy.sparse matrix to a PyTorch sparse tensor.\"\"\"\n",
    "    if not sps.isspmatrix_coo(matrix):\n",
    "        matrix = matrix.tocoo()\n",
    "\n",
    "    values = matrix.data\n",
    "    indices = np.vstack((matrix.row, matrix.col))\n",
    "\n",
    "    i = torch.LongTensor(indices)\n",
    "    v = torch.FloatTensor(values)\n",
    "    shape = matrix.shape\n",
    "\n",
    "    return torch.sparse.FloatTensor(i, v, torch.Size(shape))\n",
    "\n",
    "EJ = torch.tensor(10.00, requires_grad=True, dtype=torch.double)\n",
    "EL = torch.tensor(0.04, requires_grad=True, dtype=torch.double)\n",
    "ECs = torch.tensor(20, requires_grad=True, dtype=torch.double)\n",
    "EC = torch.tensor(0.04, requires_grad=True, dtype=torch.double)\n",
    "dEj = torch.tensor(0.0, requires_grad=True, dtype=torch.double)\n",
    "dCj = torch.tensor(0.0, requires_grad=True, dtype=torch.double)\n",
    "ECj =  torch.tensor(0.2, requires_grad=True, dtype=torch.double)\n",
    "flux = torch.tensor(0.5, requires_grad=True, dtype=torch.double)\n",
    "ng = 0.1\n",
    "ncut = 30\n",
    "truncated_dim = 10\n",
    "pt_count = 10\n",
    "min_val = -19\n",
    "max_val = 19\n",
    "hamiltonian_creation = 'auto_H'\n",
    "\n",
    "\n",
    "phi_ext = 0.5\n",
    "varphi_ext =0.5\n",
    "\n",
    "Nphi = 100\n",
    "Ntheta = 100\n",
    "\n",
    "eye_Nphi = sps.eye(Nphi)\n",
    "eye_Ntheta = sps.eye(Ntheta)\n",
    "\n",
    "partial_phi_fd = scipy_sparse_to_torch_sparse(sps.kron(eye_Ntheta, sps.diags([-1, 1, 1], [0, 1, -Nphi+1], shape=(Nphi, Nphi))))\n",
    "\n",
    "partial_phi_bk = scipy_sparse_to_torch_sparse(sps.kron(eye_Ntheta, sps.diags([1, -1, -1], [0, -1, Nphi-1], shape=(Nphi, Nphi))))\n",
    "\n",
    "partial_theta_fd = scipy_sparse_to_torch_sparse(sps.kron(eye_Nphi, sps.diags([-1, 1, 1], [0, 1, -Ntheta+1], shape=(Ntheta, Ntheta))))\n",
    "\n",
    "partial_theta_bk = scipy_sparse_to_torch_sparse(sps.kron(eye_Nphi, sps.diags([1, -1, -1], [0, -1, Ntheta-1], shape=(Ntheta, Ntheta))))\n",
    "\n",
    "\n",
    "phi = np.linspace(0, 2 * np.pi, Nphi)\n",
    "cos_phi = np.cos(phi)\n",
    "cos_phi_m = np.diag(cos_phi)\n",
    "sin_phi_adj = np.sin(phi-phi_ext/2)\n",
    "sin_phi_adj_m = np.diag(sin_phi_adj)\n",
    "phi_m = np.diag(phi)\n",
    "_cos_phi = torch.kron(torch.tensor(cos_phi_m), torch.tensor(eye_Nphi.todense()) ).to_sparse()\n",
    "_phi = torch.kron(torch.tensor(phi_m), torch.tensor(eye_Nphi.todense()) ).to_sparse()\n",
    "_sin_phi_adj_m  = torch.kron(torch.tensor(sin_phi_adj_m), torch.tensor(eye_Nphi.todense())).to_sparse()\n",
    "\n",
    "theta = np.linspace(0, 2 * np.pi, Ntheta)\n",
    "cos_theta_adj = np.cos(theta-varphi_ext/2)\n",
    "sin_theta = np.sin(theta)\n",
    "cos_theta_adj_m = np.diag(cos_theta_adj)\n",
    "sin_theta_m = np.diag(sin_theta)\n",
    "_cos_theta_adj_m = torch.kron(torch.tensor(cos_theta_adj_m), torch.tensor(eye_Ntheta.todense())).to_sparse()\n",
    "_sin_theta_m = torch.kron(torch.tensor(sin_theta_m), torch.tensor(eye_Ntheta.todense())).to_sparse()\n",
    "\n",
    "#What do the krons do? The partial derivative operators are identical  - why would we chose different N to make them different? \n",
    "\n",
    "print(partial_phi_fd.size())\n",
    "print(partial_phi_bk.size())\n",
    "print(partial_theta_fd.size())\n",
    "print(partial_theta_bk.size())\n",
    "\n",
    "print(_cos_phi .size())\n",
    "print(_phi.size())\n",
    "print(_sin_phi_adj_m.size())\n",
    "print(_cos_theta_adj_m.size())\n",
    "print(_sin_theta_m.size())\n",
    "\n",
    "\n",
    "a = -2 * ECj * (partial_phi_fd * partial_phi_bk) \n",
    "b = 2 * ECs * ((1j* partial_theta_fd.to_dense() - ng)**2).to_sparse()\n",
    "c = 2 * EJ * _cos_phi * _cos_theta_adj_m\n",
    "d = EL * _phi ** 2\n",
    "e = 2 * EJ * torch.kron(torch.tensor(eye_Nphi.todense()), torch.tensor(eye_Ntheta.todense())).to_sparse()\n",
    "f = EJ * dEj * _sin_theta_m * _sin_phi_adj_m\n",
    "print(a)\n",
    "print(a.size())\n",
    "print(b)\n",
    "print(b.size())\n",
    "print(c)\n",
    "print(c.size())\n",
    "print(d)\n",
    "print(d.size())\n",
    "print(e)\n",
    "print(e.size())\n",
    "print(f)\n",
    "print(f.size())\n",
    "\n",
    "I = torch.kron(torch.tensor(eye_Nphi.todense()), torch.tensor(eye_Ntheta.todense())).to_sparse()\n",
    "\n",
    "H = -2 * ECj * (partial_phi_fd * partial_phi_bk) \\\n",
    "    + 2 * ECs * (-1* partial_theta_fd**2 +ng**2*I-2*ng*partial_theta_fd)\\\n",
    "    + 2 * ECs * dCj * partial_phi_fd * partial_theta_fd \\\n",
    "    - 2 * EJ * _cos_phi * _cos_theta_adj_m \\\n",
    "    + EL * _phi ** 2 \\\n",
    "    + 2 * EJ * I  \\\n",
    "    + EJ * dEj * _sin_theta_m * _sin_phi_adj_m\n",
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[   0,    0,    1,  ..., 9997, 9998, 9999],\n",
       "                       [   0,    1,    1,  ..., 9998, 9999, 9900]]),\n",
       "       values=tensor([-30.5782, -40.0000, -30.5782,  ...,  -8.0000,\n",
       "                       -8.0000,  -8.0000]),\n",
       "       size=(10000, 10000), nnz=30000, dtype=torch.float64,\n",
       "       layout=torch.sparse_coo, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x,y  \u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39;49mlobpcg(H\u001b[39m.\u001b[39;49mto_dense(), k\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m )\n\u001b[1;32m      3\u001b[0m dE \u001b[39m=\u001b[39m x[\u001b[39m0\u001b[39m]\u001b[39m-\u001b[39mx[\u001b[39m1\u001b[39m]\n\u001b[1;32m      5\u001b[0m dE\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Documents/cos_potential_discre_ad/venv/lib/python3.11/site-packages/torch/_lobpcg.py:543\u001b[0m, in \u001b[0;36mlobpcg\u001b[0;34m(A, k, B, X, n, iK, niter, tol, largest, method, tracker, ortho_iparams, ortho_fparams, ortho_bparams)\u001b[0m\n\u001b[1;32m    540\u001b[0m         A_sym \u001b[39m=\u001b[39m (A \u001b[39m+\u001b[39m A\u001b[39m.\u001b[39mmT) \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m    541\u001b[0m         B_sym \u001b[39m=\u001b[39m (B \u001b[39m+\u001b[39m B\u001b[39m.\u001b[39mmT) \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m \u001b[39mif\u001b[39;00m (B \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m         \u001b[39mreturn\u001b[39;00m LOBPCGAutogradFunction\u001b[39m.\u001b[39;49mapply(\n\u001b[1;32m    544\u001b[0m             A_sym,\n\u001b[1;32m    545\u001b[0m             k,\n\u001b[1;32m    546\u001b[0m             B_sym,\n\u001b[1;32m    547\u001b[0m             X,\n\u001b[1;32m    548\u001b[0m             n,\n\u001b[1;32m    549\u001b[0m             iK,\n\u001b[1;32m    550\u001b[0m             niter,\n\u001b[1;32m    551\u001b[0m             tol,\n\u001b[1;32m    552\u001b[0m             largest,\n\u001b[1;32m    553\u001b[0m             method,\n\u001b[1;32m    554\u001b[0m             tracker,\n\u001b[1;32m    555\u001b[0m             ortho_iparams,\n\u001b[1;32m    556\u001b[0m             ortho_fparams,\n\u001b[1;32m    557\u001b[0m             ortho_bparams,\n\u001b[1;32m    558\u001b[0m         )\n\u001b[1;32m    559\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    560\u001b[0m     \u001b[39mif\u001b[39;00m A\u001b[39m.\u001b[39mrequires_grad \u001b[39mor\u001b[39;00m (B \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m B\u001b[39m.\u001b[39mrequires_grad):\n",
      "File \u001b[0;32m~/Documents/cos_potential_discre_ad/venv/lib/python3.11/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msetup_context \u001b[39m==\u001b[39m _SingleLevelFunction\u001b[39m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/cos_potential_discre_ad/venv/lib/python3.11/site-packages/torch/_lobpcg.py:283\u001b[0m, in \u001b[0;36mLOBPCGAutogradFunction.forward\u001b[0;34m(ctx, A, k, B, X, n, iK, niter, tol, largest, method, tracker, ortho_iparams, ortho_fparams, ortho_bparams)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39mif\u001b[39;00m B \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    281\u001b[0m     B \u001b[39m=\u001b[39m B\u001b[39m.\u001b[39mcontiguous() \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m B\u001b[39m.\u001b[39mis_sparse) \u001b[39melse\u001b[39;00m B\n\u001b[0;32m--> 283\u001b[0m D, U \u001b[39m=\u001b[39m _lobpcg(\n\u001b[1;32m    284\u001b[0m     A,\n\u001b[1;32m    285\u001b[0m     k,\n\u001b[1;32m    286\u001b[0m     B,\n\u001b[1;32m    287\u001b[0m     X,\n\u001b[1;32m    288\u001b[0m     n,\n\u001b[1;32m    289\u001b[0m     iK,\n\u001b[1;32m    290\u001b[0m     niter,\n\u001b[1;32m    291\u001b[0m     tol,\n\u001b[1;32m    292\u001b[0m     largest,\n\u001b[1;32m    293\u001b[0m     method,\n\u001b[1;32m    294\u001b[0m     tracker,\n\u001b[1;32m    295\u001b[0m     ortho_iparams,\n\u001b[1;32m    296\u001b[0m     ortho_fparams,\n\u001b[1;32m    297\u001b[0m     ortho_bparams,\n\u001b[1;32m    298\u001b[0m )\n\u001b[1;32m    300\u001b[0m ctx\u001b[39m.\u001b[39msave_for_backward(A, B, D, U)\n\u001b[1;32m    301\u001b[0m ctx\u001b[39m.\u001b[39mlargest \u001b[39m=\u001b[39m largest\n",
      "File \u001b[0;32m~/Documents/cos_potential_discre_ad/venv/lib/python3.11/site-packages/torch/_lobpcg.py:687\u001b[0m, in \u001b[0;36m_lobpcg\u001b[0;34m(A, k, B, X, n, iK, niter, tol, largest, method, tracker, ortho_iparams, ortho_fparams, ortho_bparams)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(X\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m X\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (m, n), (X\u001b[39m.\u001b[39mshape, (m, n))\n\u001b[1;32m    685\u001b[0m worker \u001b[39m=\u001b[39m LOBPCG(A, B, X, iK, iparams, fparams, bparams, method, tracker)\n\u001b[0;32m--> 687\u001b[0m worker\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m    689\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting():\n\u001b[1;32m    690\u001b[0m     LOBPCG\u001b[39m.\u001b[39mcall_tracker \u001b[39m=\u001b[39m LOBPCG_call_tracker_orig  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/cos_potential_discre_ad/venv/lib/python3.11/site-packages/torch/_lobpcg.py:837\u001b[0m, in \u001b[0;36mLOBPCG.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_tracker()\n\u001b[1;32m    835\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_iteration():\n\u001b[0;32m--> 837\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate()\n\u001b[1;32m    839\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting() \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtracker \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    840\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_tracker()\n",
      "File \u001b[0;32m~/Documents/cos_potential_discre_ad/venv/lib/python3.11/site-packages/torch/_lobpcg.py:767\u001b[0m, in \u001b[0;36mLOBPCG.update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mivars[\u001b[39m\"\u001b[39m\u001b[39mconverged_end\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    766\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmethod \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mortho\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 767\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_ortho()\n\u001b[1;32m    768\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    769\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_basic()\n",
      "File \u001b[0;32m~/Documents/cos_potential_discre_ad/venv/lib/python3.11/site-packages/torch/_lobpcg.py:924\u001b[0m, in \u001b[0;36mLOBPCG._update_ortho\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    922\u001b[0m S_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mS[:, nc:ns]\n\u001b[1;32m    923\u001b[0m \u001b[39m# Rayleigh-Ritz procedure\u001b[39;00m\n\u001b[0;32m--> 924\u001b[0m E_, Z \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39msymeig(_utils\u001b[39m.\u001b[39;49mqform(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mA, S_), largest)\n\u001b[1;32m    926\u001b[0m \u001b[39m# Update E, X, P\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mX[:, nc:] \u001b[39m=\u001b[39m mm(S_, Z[:, : n \u001b[39m-\u001b[39m nc])\n",
      "File \u001b[0;32m~/Documents/cos_potential_discre_ad/venv/lib/python3.11/site-packages/torch/_linalg_utils.py:74\u001b[0m, in \u001b[0;36mqform\u001b[0;34m(A, S)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mqform\u001b[39m(A: Optional[Tensor], S: Tensor):\n\u001b[1;32m     73\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return quadratic form :math:`S^T A S`.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m     \u001b[39mreturn\u001b[39;00m bform(S, A, S)\n",
      "File \u001b[0;32m~/Documents/cos_potential_discre_ad/venv/lib/python3.11/site-packages/torch/_linalg_utils.py:69\u001b[0m, in \u001b[0;36mbform\u001b[0;34m(X, A, Y)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbform\u001b[39m(X: Tensor, A: Optional[Tensor], Y: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m     68\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return bilinear form of matrices: :math:`X^T A Y`.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     \u001b[39mreturn\u001b[39;00m matmul(transpose(X), matmul(A, Y))\n",
      "File \u001b[0;32m~/Documents/cos_potential_discre_ad/venv/lib/python3.11/site-packages/torch/_linalg_utils.py:43\u001b[0m, in \u001b[0;36mmatmul\u001b[0;34m(A, B)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mif\u001b[39;00m is_sparse(A):\n\u001b[1;32m     42\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39msparse\u001b[39m.\u001b[39mmm(A, B)\n\u001b[0;32m---> 43\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mmatmul(A, B)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "x,y  =torch.lobpcg(H.to_dense(), k=10 )\n",
    "\n",
    "dE = x[0]-x[1]\n",
    "\n",
    "dE.backward()\n",
    "\n",
    "EJ.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'DominantSparseEigenAD'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mDominantSparseEigenAD\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msymeig\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msymeig\u001b[39;00m\n\u001b[1;32m      3\u001b[0m symeig\u001b[39m.\u001b[39msetDominantSparseSymeig(A, Aadjoint_to_padjoint)\n\u001b[1;32m      4\u001b[0m dominantsparsesymeig \u001b[39m=\u001b[39m symeig\u001b[39m.\u001b[39mDominantSparseSymeig\u001b[39m.\u001b[39mapply\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'DominantSparseEigenAD'"
     ]
    }
   ],
   "source": [
    "import DominantSparseEigenAD.symeig as symeig\n",
    "\n",
    "symeig.setDominantSparseSymeig(A, Aadjoint_to_padjoint)\n",
    "dominantsparsesymeig = symeig.DominantSparseSymeig.apply\n",
    "\n",
    "# Usage\n",
    "dominantsparsesymeig(EJ, 1, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jax._src.config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjax\u001b[39;00m \u001b[39mimport\u001b[39;00m grad\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjax\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mjnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m# Create a PyTorch tensor\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/optimisation_of_superconduncting_circuits/venv/lib/python3.8/site-packages/jax/__init__.py:35\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdel\u001b[39;00m _cloud_tpu_init\n\u001b[1;32m     32\u001b[0m \u001b[39m# Confusingly there are two things named \"config\": the module and the class.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39m# We want the exported object to be the class, so we first import the module\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m# to make sure a later import doesn't overwrite the class.\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjax\u001b[39;00m \u001b[39mimport\u001b[39;00m config \u001b[39mas\u001b[39;00m _config_module\n\u001b[1;32m     36\u001b[0m \u001b[39mdel\u001b[39;00m _config_module\n\u001b[1;32m     38\u001b[0m \u001b[39m# Force early import, allowing use of `jax.core` after importing `jax`.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/optimisation_of_superconduncting_circuits/venv/lib/python3.8/site-packages/jax/config.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright 2018 The JAX Authors.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[39m# TODO(phawkins): fix users of this alias and delete this file.\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjax\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_src\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconfig\u001b[39;00m \u001b[39mimport\u001b[39;00m config  \u001b[39m# noqa: F401\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'jax._src.config'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from jax import grad\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "# Create a PyTorch tensor\n",
    "torch_tensor = torch.tensor([1, 2, 3, 4, 5.])\n",
    "\n",
    "# Convert PyTorch tensor to JAX array\n",
    "jax_array = jnp.array(torch_tensor.numpy())\n",
    "\n",
    "print(jax_array)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
