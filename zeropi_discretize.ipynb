{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 10000])\n",
      "torch.Size([10000, 10000])\n",
      "torch.Size([10000, 10000])\n",
      "torch.Size([10000, 10000])\n",
      "torch.Size([10000, 10000])\n",
      "torch.Size([10000, 10000])\n",
      "torch.Size([10000, 10000])\n",
      "torch.Size([10000, 10000])\n",
      "torch.Size([10000, 10000])\n",
      "tensor(indices=tensor([[   0,    1,    2,  ..., 9997, 9998, 9999],\n",
      "                       [   0,    1,    2,  ..., 9997, 9998, 9999]]),\n",
      "       values=tensor([10., 10., 10.,  ..., 10., 10., 10.]),\n",
      "       size=(10000, 10000), nnz=10000, layout=torch.sparse_coo,\n",
      "       grad_fn=<MulBackward0>)\n",
      "torch.Size([10000, 10000])\n",
      "tensor(indices=tensor([[   0,    0,    0,  ..., 9999, 9999, 9999],\n",
      "                       [   0,    1,    2,  ..., 9997, 9998, 9999]]),\n",
      "       values=tensor([ 0.+20.j,  0.-20.j, 10.+0.j,  ..., 10.+0.j,\n",
      "                      10.+0.j,  0.+20.j]),\n",
      "       size=(10000, 10000), nnz=100000000, layout=torch.sparse_coo,\n",
      "       grad_fn=<MulBackward0>)\n",
      "torch.Size([10000, 10000])\n",
      "tensor(indices=tensor([[   0,    1,    2,  ..., 9997, 9998, 9999],\n",
      "                       [   0,    1,    2,  ..., 9997, 9998, 9999]]),\n",
      "       values=tensor([9.6891, 9.6891, 9.6891,  ..., 9.6891, 9.6891, 9.6891]),\n",
      "       size=(10000, 10000), nnz=10000, dtype=torch.float64,\n",
      "       layout=torch.sparse_coo, grad_fn=<MulBackward0>)\n",
      "torch.Size([10000, 10000])\n",
      "tensor(indices=tensor([[ 100,  101,  102,  ..., 9997, 9998, 9999],\n",
      "                       [ 100,  101,  102,  ..., 9997, 9998, 9999]]),\n",
      "       values=tensor([2.4168e-02, 2.4168e-02, 2.4168e-02,  ...,\n",
      "                      2.3687e+02, 2.3687e+02, 2.3687e+02]),\n",
      "       size=(10000, 10000), nnz=9900, dtype=torch.float64,\n",
      "       layout=torch.sparse_coo, grad_fn=<MulBackward0>)\n",
      "torch.Size([10000, 10000])\n",
      "tensor(indices=tensor([[   0,    1,    2,  ..., 9997, 9998, 9999],\n",
      "                       [   0,    1,    2,  ..., 9997, 9998, 9999]]),\n",
      "       values=tensor([10., 10., 10.,  ..., 10., 10., 10.]),\n",
      "       size=(10000, 10000), nnz=10000, dtype=torch.float64,\n",
      "       layout=torch.sparse_coo, grad_fn=<MulBackward0>)\n",
      "torch.Size([10000, 10000])\n",
      "tensor(indices=tensor([[ 100,  101,  102,  ..., 9997, 9998, 9999],\n",
      "                       [ 100,  101,  102,  ..., 9997, 9998, 9999]]),\n",
      "       values=tensor([-2.9405e-01, -2.9405e-01, -2.9405e-01,  ...,\n",
      "                       1.5149e-15,  1.5149e-15,  1.5149e-15]),\n",
      "       size=(10000, 10000), nnz=9900, dtype=torch.float64,\n",
      "       layout=torch.sparse_coo, grad_fn=<MulBackward0>)\n",
      "torch.Size([10000, 10000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[   0,    0,    1,  ..., 9997, 9998, 9999],\n",
       "                       [   0,    1,    1,  ..., 9998, 9999, 9900]]),\n",
       "       values=tensor([ 80.3109, -10.0000,  80.3109,  ...,  30.0000,\n",
       "                       30.0000,  30.0000]),\n",
       "       size=(10000, 10000), nnz=30000, dtype=torch.float64,\n",
       "       layout=torch.sparse_coo, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy as sp\n",
    "import scipy.sparse as sps\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def scipy_sparse_to_torch_sparse(matrix):\n",
    "    \"\"\"Converts a scipy.sparse matrix to a PyTorch sparse tensor.\"\"\"\n",
    "    if not sps.isspmatrix_coo(matrix):\n",
    "        matrix = matrix.tocoo()\n",
    "\n",
    "    values = matrix.data\n",
    "    indices = np.vstack((matrix.row, matrix.col))\n",
    "\n",
    "    i = torch.LongTensor(indices)\n",
    "    v = torch.FloatTensor(values)\n",
    "    shape = matrix.shape\n",
    "\n",
    "    return torch.sparse.FloatTensor(i, v, torch.Size(shape))\n",
    "\n",
    "EJ = torch.tensor(10.00, requires_grad=True, dtype=torch.double)\n",
    "EL = torch.tensor(0.04, requires_grad=True, dtype=torch.double)\n",
    "ECJ = torch.tensor(20, requires_grad=True, dtype=torch.double)\n",
    "EC = torch.tensor(0.04, requires_grad=True, dtype=torch.double)\n",
    "dEJ = torch.tensor(0.0, requires_grad=True, dtype=torch.double)\n",
    "dCJ = torch.tensor(0.0, requires_grad=True, dtype=torch.double)\n",
    "flux = torch.tensor(0.5, requires_grad=True, dtype=torch.double)\n",
    "ng = 0.1\n",
    "ncut = 30\n",
    "truncated_dim = 10\n",
    "pt_count = 10\n",
    "min_val = -19\n",
    "max_val = 19\n",
    "hamiltonian_creation = 'auto_H'\n",
    "\n",
    "\n",
    "phi_ext = 0.5\n",
    "varphi_ext =0.5\n",
    "\n",
    "Nphi = 100\n",
    "Ntheta = 100\n",
    "\n",
    "eye_Nphi = sps.eye(Nphi)\n",
    "eye_Ntheta = sps.eye(Ntheta)\n",
    "\n",
    "partial_phi_fd = scipy_sparse_to_torch_sparse(sps.kron(eye_Ntheta, sps.diags([-1, 1, 1], [0, 1, -Nphi+1], shape=(Nphi, Nphi))))\n",
    "\n",
    "partial_phi_bk = scipy_sparse_to_torch_sparse(sps.kron(eye_Ntheta, sps.diags([1, -1, -1], [0, -1, Nphi-1], shape=(Nphi, Nphi))))\n",
    "\n",
    "partial_theta_fd = scipy_sparse_to_torch_sparse(sps.kron(eye_Nphi, sps.diags([-1, 1, 1], [0, 1, -Ntheta+1], shape=(Ntheta, Ntheta))))\n",
    "\n",
    "partial_theta_bk = scipy_sparse_to_torch_sparse(sps.kron(eye_Nphi, sps.diags([1, -1, -1], [0, -1, Ntheta-1], shape=(Ntheta, Ntheta))))\n",
    "\n",
    "\n",
    "phi = np.linspace(0, 2 * np.pi, Nphi)\n",
    "cos_phi = np.cos(phi)\n",
    "cos_phi_m = np.diag(cos_phi)\n",
    "sin_phi_adj = np.sin(phi-phi_ext/2)\n",
    "sin_phi_adj_m = np.diag(sin_phi_adj)\n",
    "phi_m = np.diag(phi)\n",
    "_cos_phi = torch.kron(torch.tensor(cos_phi_m), torch.tensor(eye_Nphi.todense()) ).to_sparse()\n",
    "_phi = torch.kron(torch.tensor(phi_m), torch.tensor(eye_Nphi.todense()) ).to_sparse()\n",
    "_sin_phi_adj_m  = torch.kron(torch.tensor(sin_phi_adj_m), torch.tensor(eye_Nphi.todense())).to_sparse()\n",
    "\n",
    "theta = np.linspace(0, 2 * np.pi, Ntheta)\n",
    "cos_theta_adj = np.cos(theta-varphi_ext/2)\n",
    "sin_theta = np.sin(theta)\n",
    "cos_theta_adj_m = np.diag(cos_theta_adj)\n",
    "sin_theta_m = np.diag(sin_theta)\n",
    "_cos_theta_adj_m = torch.kron(torch.tensor(cos_theta_adj_m), torch.tensor(eye_Ntheta.todense())).to_sparse()\n",
    "_sin_theta_m = torch.kron(torch.tensor(sin_theta_m), torch.tensor(eye_Ntheta.todense())).to_sparse()\n",
    "\n",
    "#What do the krons do? The partial derivative operators are identical  - why would we chose different N to make them different? \n",
    "\n",
    "print(partial_phi_fd.size())\n",
    "print(partial_phi_bk.size())\n",
    "print(partial_theta_fd.size())\n",
    "print(partial_theta_bk.size())\n",
    "\n",
    "print(_cos_phi .size())\n",
    "print(_phi.size())\n",
    "print(_sin_phi_adj_m.size())\n",
    "print(_cos_theta_adj_m.size())\n",
    "print(_sin_theta_m.size())\n",
    "\n",
    "\n",
    "a = -2 * ECj * (partial_phi_fd * partial_phi_bk) \n",
    "b = 2 * ECs * ((1j* partial_theta_fd.to_dense() - ng)**2).to_sparse()\n",
    "c = 2 * EJ * _cos_phi * _cos_theta_adj_m\n",
    "d = EL * _phi ** 2\n",
    "e = 2 * EJ * torch.kron(torch.tensor(eye_Nphi.todense()), torch.tensor(eye_Ntheta.todense())).to_sparse()\n",
    "f = EJ * dEj * _sin_theta_m * _sin_phi_adj_m\n",
    "print(a)\n",
    "print(a.size())\n",
    "print(b)\n",
    "print(b.size())\n",
    "print(c)\n",
    "print(c.size())\n",
    "print(d)\n",
    "print(d.size())\n",
    "print(e)\n",
    "print(e.size())\n",
    "print(f)\n",
    "print(f.size())\n",
    "\n",
    "I = torch.kron(torch.tensor(eye_Nphi.todense()), torch.tensor(eye_Ntheta.todense())).to_sparse()\n",
    "\n",
    "H = -2 * ECj * (partial_phi_fd * partial_phi_bk) \\\n",
    "    + 2 * ECs * (-1* partial_theta_fd**2 +ng**2*I-2*ng*partial_theta_fd)\\\n",
    "    + 2 * ECs * dCj * partial_phi_fd * partial_theta_fd \\\n",
    "    - 2 * EJ * _cos_phi * _cos_theta_adj_m \\\n",
    "    + EL * _phi ** 2 \\\n",
    "    + 2 * EJ * I  \\\n",
    "    + EJ * dEj * _sin_theta_m * _sin_phi_adj_m\n",
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 59.86364592,  59.92336691,  60.13384036, ..., 329.33835129,\n",
       "        333.10479139, 337.18138141]),\n",
       " array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        , -0.70710678]]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy as sp\n",
    "sp.linalg.eigh(H.to_dense().detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "_LinAlgError",
     "evalue": "linalg.cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 9816 is not positive-definite).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_LinAlgError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m x,y  \u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlobpcg(H\u001b[39m.\u001b[39mto_dense(), k\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m )\n\u001b[1;32m      3\u001b[0m dE \u001b[39m=\u001b[39m x[\u001b[39m0\u001b[39m]\u001b[39m-\u001b[39mx[\u001b[39m1\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m dE\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m      7\u001b[0m EJ\u001b[39m.\u001b[39mgrad\n",
      "File \u001b[0;32m~/Documents/optimisation_of_superconduncting_circuits/venv/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/optimisation_of_superconduncting_circuits/venv/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/Documents/optimisation_of_superconduncting_circuits/venv/lib/python3.8/site-packages/torch/autograd/function.py:274\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImplementing both \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbackward\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvjp\u001b[39m\u001b[39m'\u001b[39m\u001b[39m for a custom \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    271\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mFunction is not allowed. You should only implement one \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    272\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mof them.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    273\u001b[0m user_fn \u001b[39m=\u001b[39m vjp_fn \u001b[39mif\u001b[39;00m vjp_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m Function\u001b[39m.\u001b[39mvjp \u001b[39melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 274\u001b[0m \u001b[39mreturn\u001b[39;00m user_fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/Documents/optimisation_of_superconduncting_circuits/venv/lib/python3.8/site-packages/torch/_lobpcg.py:338\u001b[0m, in \u001b[0;36mLOBPCGAutogradFunction.backward\u001b[0;34m(ctx, D_grad, U_grad)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[39m# symeig backward\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[39mif\u001b[39;00m B \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 338\u001b[0m     A_grad \u001b[39m=\u001b[39m _symeig_backward(D_grad, U_grad, A, D, U, largest)\n\u001b[1;32m    340\u001b[0m \u001b[39m# A has index 0\u001b[39;00m\n\u001b[1;32m    341\u001b[0m grads[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m A_grad\n",
      "File \u001b[0;32m~/Documents/optimisation_of_superconduncting_circuits/venv/lib/python3.8/site-packages/torch/_lobpcg.py:254\u001b[0m, in \u001b[0;36m_symeig_backward\u001b[0;34m(D_grad, U_grad, A, D, U, largest)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[39mreturn\u001b[39;00m _symeig_backward_complete_eigenspace(D_grad, U_grad, A, D, U)\n\u001b[1;32m    253\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 254\u001b[0m     \u001b[39mreturn\u001b[39;00m _symeig_backward_partial_eigenspace(D_grad, U_grad, A, D, U, largest)\n",
      "File \u001b[0;32m~/Documents/optimisation_of_superconduncting_circuits/venv/lib/python3.8/site-packages/torch/_lobpcg.py:230\u001b[0m, in \u001b[0;36m_symeig_backward_partial_eigenspace\u001b[0;34m(D_grad, U_grad, A, D, U, largest)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[39m# we need to invert 'chr_poly_D_at_A_to_U_ortho`, for that we compute its\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[39m# Cholesky decomposition and then use `torch.cholesky_solve` for better stability.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[39m# Cholesky decomposition requires the input to be positive-definite.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[39m# check if `chr_poly_D_at_A_to_U_ortho` is positive-definite or negative-definite\u001b[39;00m\n\u001b[1;32m    229\u001b[0m chr_poly_D_at_A_to_U_ortho_sign \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m (largest \u001b[39mand\u001b[39;00m (k \u001b[39m%\u001b[39m \u001b[39m2\u001b[39m \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m)) \u001b[39melse\u001b[39;00m \u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m--> 230\u001b[0m chr_poly_D_at_A_to_U_ortho_L \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49mcholesky(\n\u001b[1;32m    231\u001b[0m     chr_poly_D_at_A_to_U_ortho_sign \u001b[39m*\u001b[39;49m chr_poly_D_at_A_to_U_ortho\n\u001b[1;32m    232\u001b[0m )\n\u001b[1;32m    234\u001b[0m \u001b[39m# compute the gradient part in span(U)\u001b[39;00m\n\u001b[1;32m    235\u001b[0m res \u001b[39m=\u001b[39m _symeig_backward_complete_eigenspace(D_grad, U_grad, A, D, U)\n",
      "\u001b[0;31m_LinAlgError\u001b[0m: linalg.cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 9816 is not positive-definite)."
     ]
    }
   ],
   "source": [
    "\n",
    "x,y  =torch.lobpcg(H.to_dense(), k=10 )\n",
    "\n",
    "dE = x[0]-x[1]\n",
    "\n",
    "dE.backward()\n",
    "\n",
    "EJ.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DominantSparseEigenAD.symeig as symeig\n",
    "\n",
    "symeig.setDominantSparseSymeig(A, Aadjoint_to_padjoint)\n",
    "dominantsparsesymeig = symeig.DominantSparseSymeig.apply\n",
    "\n",
    "# Usage\n",
    "dominantsparsesymeig(EJ, 1, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jax._src.config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjax\u001b[39;00m \u001b[39mimport\u001b[39;00m grad\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjax\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mjnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m# Create a PyTorch tensor\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/optimisation_of_superconduncting_circuits/venv/lib/python3.8/site-packages/jax/__init__.py:35\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdel\u001b[39;00m _cloud_tpu_init\n\u001b[1;32m     32\u001b[0m \u001b[39m# Confusingly there are two things named \"config\": the module and the class.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39m# We want the exported object to be the class, so we first import the module\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m# to make sure a later import doesn't overwrite the class.\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjax\u001b[39;00m \u001b[39mimport\u001b[39;00m config \u001b[39mas\u001b[39;00m _config_module\n\u001b[1;32m     36\u001b[0m \u001b[39mdel\u001b[39;00m _config_module\n\u001b[1;32m     38\u001b[0m \u001b[39m# Force early import, allowing use of `jax.core` after importing `jax`.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/optimisation_of_superconduncting_circuits/venv/lib/python3.8/site-packages/jax/config.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright 2018 The JAX Authors.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[39m# TODO(phawkins): fix users of this alias and delete this file.\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjax\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_src\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconfig\u001b[39;00m \u001b[39mimport\u001b[39;00m config  \u001b[39m# noqa: F401\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'jax._src.config'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from jax import grad\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "# Create a PyTorch tensor\n",
    "torch_tensor = torch.tensor([1, 2, 3, 4, 5.])\n",
    "\n",
    "# Convert PyTorch tensor to JAX array\n",
    "jax_array = jnp.array(torch_tensor.numpy())\n",
    "\n",
    "print(jax_array)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
